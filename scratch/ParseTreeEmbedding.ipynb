{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse.csgraph as csg\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import networkx as nx\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import math\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import learning_util as lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distortion calculations\n",
    "\n",
    "def acosh(x):\n",
    "    return torch.log(x + torch.sqrt(x**2-1))\n",
    "\n",
    "def dist_h(u,v):\n",
    "    z  = 2*torch.norm(u-v,2)**2\n",
    "    uu = 1. + torch.div(z,((1-torch.norm(u,2)**2)*(1-torch.norm(v,2)**2)))\n",
    "    return acosh(uu)\n",
    "\n",
    "def distance_matrix_euclidean(input):\n",
    "    row_n = input.shape[0]\n",
    "    mp1 = torch.stack([input]*row_n)\n",
    "    mp2 = torch.stack([input]*row_n).transpose(0,1)\n",
    "    dist_mat = torch.sum((mp1-mp2)**2,2).squeeze()\n",
    "    return dist_mat\n",
    "\n",
    "def distance_matrix_hyperbolic(input):\n",
    "    row_n = input.shape[0]\n",
    "    dist_mat = torch.zeros(row_n, row_n, device=device)\n",
    "    for row in range(row_n):\n",
    "        for i in range(row_n):\n",
    "            if i != row:\n",
    "                dist_mat[row, i] = dist_h(input[row,:], input[i,:])\n",
    "    return dist_mat\n",
    "\n",
    "def entry_is_good(h, h_rec): return (not torch.isnan(h_rec)) and (not torch.isinf(h_rec)) and h_rec != 0 and h != 0\n",
    "\n",
    "def distortion_entry(h,h_rec):\n",
    "    avg = abs(h_rec - h)/h\n",
    "    return avg\n",
    "\n",
    "def distortion_row(H1, H2, n, row):\n",
    "    avg, good = 0, 0\n",
    "    for i in range(n):\n",
    "        if i != row and entry_is_good(H1[i], H2[i]):\n",
    "            _avg = distortion_entry(H1[i], H2[i])\n",
    "            good        += 1\n",
    "            avg         += _avg\n",
    "    if good > 0:\n",
    "        avg /= good \n",
    "    else:\n",
    "        avg, good = torch.tensor(0., device=device, requires_grad=True), torch.tensor(0., device=device, requires_grad=True)\n",
    "    return (avg, good)\n",
    "\n",
    "def distortion(H1, H2, n, jobs=16):\n",
    "#     dists = Parallel(n_jobs=jobs)(delayed(distortion_row)(H1[i,:],H2[i,:],n,i) for i in range(n))\n",
    "    dists = (distortion_row(H1[i,:],H2[i,:],n,i) for i in range(n))\n",
    "    to_stack = [tup[0] for tup in dists]\n",
    "    avg = torch.stack(to_stack).sum()/n\n",
    "    return avg\n",
    "\n",
    "\n",
    "#Loading the graph and getting the distance matrix.\n",
    "\n",
    "def load_graph(file_name, directed=False):\n",
    "    G = nx.DiGraph() if directed else nx.Graph()\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.split()\n",
    "            u = int(tokens[0])\n",
    "            v = int(tokens[1])\n",
    "            if len(tokens) > 2:\n",
    "                w = float(tokens[2])\n",
    "                G.add_edge(u, v, weight=w)\n",
    "            else:\n",
    "                G.add_edge(u,v)\n",
    "    return G\n",
    "\n",
    "\n",
    "def compute_row(i, adj_mat): \n",
    "    return csg.dijkstra(adj_mat, indices=[i], unweighted=True, directed=False)\n",
    "\n",
    "def get_dist_mat(G):\n",
    "    n = G.order()\n",
    "    adj_mat = nx.to_scipy_sparse_matrix(G, nodelist=list(range(G.order())))\n",
    "    t = time.time()\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    dist_mat = Parallel(n_jobs=num_cores)(delayed(compute_row)(i,adj_mat) for i in range(n))\n",
    "    dist_mat = np.vstack(dist_mat)\n",
    "    return dist_mat\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for token in sentence:\n",
    "            self.addWord(token['form'])\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_tree, parse_tree_incr, parse, parse_incr\n",
    "from io import open\n",
    "import scipy.sparse.csgraph as csg\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import string\n",
    "\n",
    "\n",
    "def unroll(node, G):\n",
    "    if len(node.children) != 0:\n",
    "        for child in node.children:\n",
    "            G.add_edge(node.token['id'], child.token['id'])\n",
    "            unroll(child, G)\n",
    "    return G\n",
    "\n",
    "sentences = []\n",
    "data_file = open(\"UD_English-EWT/en_ewt-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "for sentence in parse_incr(data_file):\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "MIN_LENGTH = 10\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "def check_length(sentence):\n",
    "    return len(sentence) < MAX_LENGTH and len(sentence) > MIN_LENGTH \n",
    "\n",
    "def filterSentences(sentences):\n",
    "    return [sent for sent in sentences if check_length(sent)]\n",
    "\n",
    "input_vocab = Vocab(\"ewt_train_trimmed\")\n",
    "filtered_sentences = filterSentences(sentences)\n",
    "\n",
    "sentences_text = []\n",
    "for sent in filtered_sentences:\n",
    "    input_vocab.addSentence(sent)\n",
    "    sentences_text.append(sent.metadata['text'])\n",
    "    \n",
    "dev_dict  = {}\n",
    "for idx in range(0, len(filtered_sentences)):\n",
    "    curr_tree = filtered_sentences[idx].to_tree()\n",
    "    G_curr = nx.Graph()\n",
    "    G_curr = unroll(curr_tree, G_curr)\n",
    "    G = nx.relabel_nodes(G_curr, lambda x: x-1)\n",
    "    nx.write_edgelist(G, \"train/\"+str(idx)+\".edges\", data=False)\n",
    "    G_final = nx.convert_node_labels_to_integers(G_curr, ordering = \"decreasing degree\")\n",
    "    nx.write_edgelist(G_final, \"ewt_train/\"+str(idx)+\".edges\", data=False)\n",
    "    dev_dict[idx] = list(G_final.edges)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(vocab, sentence):\n",
    "    return [vocab.word2index[token['form']] for token in sentence]\n",
    "\n",
    "def tensorFromSentence(vocab, sentence):\n",
    "    indexes = indexesFromSentence(vocab, sentence)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def pairfromidx(idx):\n",
    "    input_tensor = tensorFromSentence(input_vocab, filtered_sentences[idx])\n",
    "    G = load_graph(\"train/\"+str(idx)+\".edges\")\n",
    "    target_matrix = get_dist_mat(G)\n",
    "    target_tensor = torch.from_numpy(target_matrix).float().to(device)\n",
    "    target_tensor.requires_grad = False\n",
    "    n = G.order()\n",
    "    return (input_tensor, target_tensor, n, sentences_text[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    \n",
    "class HyperbolicEncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(HyperbolicEncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = HyperbolicLSTM(cell_class=HyperbolicLSTMCell, input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(Attention, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        attention_scores = self.attn(torch.cat((embedded[0], hidden.unsqueeze(0)), 1))\n",
    "        attn_weights = F.softmax(attention_scores, dim=0)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperbolic modules.\n",
    "\n",
    "class HypLinear(nn.Module):\n",
    "    \"\"\"Applies a hyperbolic \"linear\" transformation to the incoming data: :math:`y = xA^T + b`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(HypLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(1, out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        result = lu.torch_hyp_add(lu.torch_mv_mul_hyp(torch.transpose(self.weight,0,1), input_), self.bias) #(batch, input) x (input, output)\n",
    "        return result\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainVanilla(input_tensor, ground_truth, n, encoder, encoder_optimizer, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    " \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = ground_truth.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    final_embeddings = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    dist_recovered = distance_matrix_euclidean(encoder_outputs)\n",
    "    loss += distortion(ground_truth, dist_recovered, n)\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWAttention(input_tensor, ground_truth, n, encoder, encoder_optimizer, attention, attention_optimizer, iter, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    attention_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = ground_truth.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    encoder_hiddens = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "    final_embeddings = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        encoder_hiddens[ei] = encoder_hidden[0, 0]\n",
    "        \n",
    "    for idx in range(input_length):\n",
    "        output = attention(input_tensor[idx], encoder_hiddens[idx], encoder_outputs)\n",
    "        final_embeddings[idx] = output[0]\n",
    "        \n",
    "    dist_recovered = distance_matrix_euclidean(final_embeddings)\n",
    "    loss += distortion(ground_truth, dist_recovered, n)\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    attention_optimizer.step()\n",
    "\n",
    "    return loss.item(), final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEuclidean(encoder, attention, n_iters=7600, print_every=100, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    attention_optimizer = optim.SGD(attention.parameters(), lr=learning_rate)\n",
    "    training_pairs = [pairfromidx(idx) for idx in range(n_iters)]\n",
    "\n",
    "    euclidean_emb_dict = {}\n",
    "    for iter in range(1, n_iters+1):     \n",
    "        training_pair = training_pairs[iter]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_matrix = training_pair[1]\n",
    "        n = training_pair[2]\n",
    "        loss, final_embeddings = trainWAttention(input_tensor, target_matrix, n, encoder, encoder_optimizer, attention, attention_optimizer, iter-1)\n",
    "        torch.save(final_embeddings, \"saved_tensors/\"+str(iter-1)+\".pt\")\n",
    "        euclidean_emb_dict[iter-1] = final_embeddings\n",
    "#         loss = train(input_tensor, target_matrix, n, encoder, encoder_optimizer)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    return euclidean_emb_dict\n",
    "\n",
    "hidden_size = 100\n",
    "encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
    "attention = Attention(input_vocab.n_words, hidden_size).to(device)\n",
    "euclidean_emb_dict = trainEuclidean(encoder, attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_embeddings = {}\n",
    "saved_tensors = os.listdir(\"saved_tensors/\")\n",
    "for file in saved_tensors:\n",
    "    idx = int(file.split(\".\")[0])\n",
    "    euclidean_embeddings[idx] = torch.load(\"saved_tensors/\"+str(file), map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Riemannian SGD\n",
    "\n",
    "# import glob\n",
    "# from torch.optim import Optimizer\n",
    "\n",
    "# class RiemannianSGD(Optimizer):\n",
    "#     \"\"\"Riemannian stochastic gradient descent.\n",
    "#     Args:\n",
    "#         params (iterable): iterable of parameters to optimize or dicts defining\n",
    "#             parameter groups\n",
    "#         lr (float): learning rate\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, params, lr):\n",
    "#         # if lr is not required and lr < 0.0:\n",
    "#         #     raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "#         defaults = dict(lr=lr)\n",
    "#         super(RiemannianSGD, self).__init__(params, defaults)\n",
    "\n",
    "#     def step(self):\n",
    "#         \"\"\"Performs a single optimization step.\n",
    "#         Arguments:\n",
    "#             lr (float): learning rate for the current update.\n",
    "#         \"\"\"\n",
    "#         loss = None\n",
    "\n",
    "#         for group in self.param_groups:\n",
    "#             for p in group['params']:\n",
    "#                 if p.grad is None:\n",
    "#                     continue\n",
    "#                 d_p = p.grad.data\n",
    "#                 lr = group['lr']\n",
    "       \n",
    "#             if torch.all(p.grad > -1e4) and torch.all(p.grad < 1e4):\n",
    "#                 p.data.add_(hyperbolic_step(p.data, d_p, lr))\n",
    "\n",
    "#         return loss\n",
    "\n",
    "# def batch_dot(u, v):\n",
    "#     return torch.sum(u * v, dim=-1, keepdim=True)\n",
    "\n",
    "# def natural_grad(v, dv):\n",
    "#     vnorm_squared = batch_dot(v, v)\n",
    "#     dv = dv * ((1 - vnorm_squared) ** 2 / 4).expand_as(dv)\n",
    "#     return dv\n",
    "\n",
    "# def batch_add(u, v, c=1):\n",
    "#     numer = 1 + 2 * batch_dot(u, v) + batch_dot(v, v) * u + (1 - batch_dot(u, u)) * v\n",
    "#     denom = 1 + 2 * batch_dot(u, v) + batch_dot(v, v) * batch_dot(u, u)\n",
    "\n",
    "#     return numer/denom\n",
    "\n",
    "# def batch_exp_map(x, v, c=1):\n",
    "#     term = torch.tanh((torch.norm(v, dim=-1, keepdim=True) / (1 - torch.norm(x, dim=-1, keepdim=True).pow(2)))) * \\\n",
    "#                  (v/(torch.norm(v, dim=-1, keepdim=True)))\n",
    "#     return batch_add(x, term, c)\n",
    "\n",
    "# def hyperbolic_step(param, grad, lr):\n",
    "#     ngrad = natural_grad(param, grad)\n",
    "#     return batch_exp_map(param, -lr * ngrad, c=1)\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "spten_t = torch.sparse.FloatTensor\n",
    "\n",
    "\n",
    "def poincare_grad(p, d_p):\n",
    "    r\"\"\"\n",
    "    Function to compute Riemannian gradient from the\n",
    "    Euclidean gradient in the PoincarÃ© ball.\n",
    "    Args:\n",
    "        p (Tensor): Current point in the ball\n",
    "        d_p (Tensor): Euclidean gradient at p\n",
    "    \"\"\"\n",
    "    if d_p.is_sparse:\n",
    "        p_sqnorm = torch.sum(\n",
    "            p.data[d_p._indices()[0].squeeze()] ** 2, dim=1,\n",
    "            keepdim=True\n",
    "        ).expand_as(d_p._values())\n",
    "        n_vals = d_p._values() * ((1 - p_sqnorm) ** 2) / 4\n",
    "        d_p = spten_t(d_p._indices(), n_vals, d_p.size())\n",
    "    else:\n",
    "        p_sqnorm = torch.sum(p.data ** 2, dim=-1, keepdim=True)\n",
    "        d_p = d_p * ((1 - p_sqnorm) ** 2 / 4).expand_as(d_p)\n",
    "\n",
    "    return d_p\n",
    "\n",
    "\n",
    "def euclidean_grad(p, d_p):\n",
    "    return d_p\n",
    "\n",
    "\n",
    "def retraction(p, d_p, lr):\n",
    "    if torch.all(d_p < 100) and torch.all(d_p>-100):\n",
    "        p.data.add_(-lr, d_p)\n",
    "\n",
    "\n",
    "class RiemannianSGD(Optimizer):\n",
    "    r\"\"\"Riemannian stochastic gradient descent.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        rgrad (Function): Function to compute the Riemannian gradient from\n",
    "            an Euclidean gradient\n",
    "        retraction (Function): Function to update the parameters via a\n",
    "            retraction of the Riemannian gradient\n",
    "        lr (float): learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=required, rgrad=required, retraction=required):\n",
    "        defaults = dict(lr=lr, rgrad=rgrad, retraction=retraction)\n",
    "        super(RiemannianSGD, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, lr=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            lr (float, optional): learning rate for the current update.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if lr is None:\n",
    "                    lr = group['lr']\n",
    "                d_p = group['rgrad'](p, d_p)\n",
    "                group['retraction'](p, d_p, lr)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Euclidean to hyperbolic mapping in one FC layer. (using GT)\n",
    "\n",
    "def trainFCHyp(euclidean_embs, ground_truth, n, fc, fc_optimizer, max_length=MAX_LENGTH):\n",
    "    fc_optimizer.zero_grad()\n",
    " \n",
    "    final_embeddings = torch.zeros(fc.in_features, fc.out_features, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    for idx in range(fc.in_features):\n",
    "        output = fc(euclidean_embs[idx])\n",
    "        final_embeddings[idx] = output[0]\n",
    "\n",
    "    dist_recovered = distance_matrix_hyperbolic(final_embeddings) \n",
    "    loss += distortion(ground_truth, dist_recovered, n)\n",
    "    loss.backward()\n",
    "    fc_optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3m 49s (- 26m 49s) (100 12%) 0.5048\n",
      "7m 31s (- 22m 33s) (200 25%) 0.5074\n",
      "20m 41s (- 34m 28s) (300 37%) 0.5064\n",
      "94m 6s (- 94m 6s) (400 50%) 0.4960\n",
      "97m 48s (- 58m 40s) (500 62%) 0.5023\n",
      "101m 30s (- 33m 50s) (600 75%) 0.4972\n",
      "105m 12s (- 15m 1s) (700 87%) 0.4920\n",
      "108m 54s (- 0m 0s) (800 100%) 0.4908\n"
     ]
    }
   ],
   "source": [
    "def trainFCIters(fc, n_iters=800, print_every=100, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "\n",
    "    fc_optimizer = RiemannianSGD(fc.parameters(), lr=learning_rate, rgrad=poincare_grad, retraction=euclidean_retraction)\n",
    "\n",
    "    training_pairs = [pairfromidx(idx) for idx in range(n_iters)]\n",
    "\n",
    "    for iter in range(1, n_iters + 1):     \n",
    "        input_matrix = euclidean_embeddings[iter - 1]\n",
    "        target_matrix = training_pairs[iter-1][1]\n",
    "        n = training_pairs[iter-1][2]\n",
    "        loss = trainFCHyp(euclidean_embeddings, target_matrix, n, fc, fc_optimizer)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "input_size = 100\n",
    "output_size = 10\n",
    "fc = nn.Linear(input_size, output_size).to(device)\n",
    "trainFCIters(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def dot(x,y): return torch.sum(x * y, -1)\n",
    "def acosh(x):\n",
    "    return torch.log(x + torch.sqrt(x**2-1))\n",
    "\n",
    "\n",
    "class RParameter(nn.Parameter):\n",
    "    def __new__(cls, data=None, requires_grad=True, sizes=None, exp=False):\n",
    "        if data is None:\n",
    "            assert sizes is not None\n",
    "            data = (1e-3 * torch.randn(sizes, dtype=torch.double)).clamp_(min=-3e-3,max=3e-3)\n",
    "        #TODO get partial data if too big i.e. data[0:n,0:d]\n",
    "        ret =  super().__new__(cls, data, requires_grad=requires_grad)\n",
    "        # ret.data    = data\n",
    "        ret.initial_proj()\n",
    "        ret.use_exp = exp\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def _proj(x):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def proj(self):\n",
    "        self.data = self.__class__._proj(self.data.detach())\n",
    "        # print(torch.norm(self.data, dim=-1))\n",
    "\n",
    "    def initial_proj(self):\n",
    "        \"\"\" Project the initialization of the embedding onto the manifold \"\"\"\n",
    "        self.proj()\n",
    "\n",
    "    def modify_grad_inplace(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def correct_metric(ps):\n",
    "        for p in ps:\n",
    "            if isinstance(p,RParameter):\n",
    "                p.modify_grad_inplace()\n",
    "\n",
    "\n",
    "# TODO can use kwargs instead of pasting defaults\n",
    "class HyperboloidParameter(RParameter):\n",
    "    def __new__(cls, data=None, requires_grad=True, sizes=None, exp=True):\n",
    "        if sizes is not None:\n",
    "            sizes = list(sizes)\n",
    "            sizes[-1] += 1\n",
    "        return super().__new__(cls, data, requires_grad, sizes, exp)\n",
    "\n",
    "    @staticmethod\n",
    "    def dot_h(x,y):\n",
    "        return torch.sum(x * y, -1) - 2*x[...,0]*y[...,0]\n",
    "    @staticmethod\n",
    "    def norm_h(x):\n",
    "        assert torch.all(HyperboloidParameter.dot_h(x,x) >= 0), torch.min(HyperboloidParameter.dot_h(x,x))\n",
    "        return torch.sqrt(torch.clamp(HyperboloidParameter.dot_h(x,x), min=0.0))\n",
    "    @staticmethod\n",
    "    def dist_h(x,y):\n",
    "        bad = torch.min(-HyperboloidParameter.dot_h(x,y) - 1.0)\n",
    "        if bad <= -1e-4:\n",
    "            print(\"bad dist\", bad.item())\n",
    "        return acosh(torch.clamp(-HyperboloidParameter.dot_h(x,y), min=(1.0+1e-8)))\n",
    "\n",
    "    @staticmethod\n",
    "    def _proj(x):\n",
    "        \"\"\" Project onto hyperboloid \"\"\"\n",
    "        x_ = torch.tensor(x)\n",
    "        x_tail = x_[...,1:]\n",
    "        current_norms = torch.norm(x_tail,2,-1)\n",
    "        scale      = (current_norms/1e7).clamp_(min=1.0)\n",
    "        x_tail /= scale.unsqueeze(-1)\n",
    "        x_[...,1:] = x_tail\n",
    "        x_[...,0] = torch.sqrt(1 + torch.norm(x_tail,2,-1)**2)\n",
    "\n",
    "        debug = True\n",
    "        if debug:\n",
    "            bad = torch.min(-HyperboloidParameter.dot_h(x_,x_))\n",
    "            if bad <= 0.0:\n",
    "                print(\"way off hyperboloid\", bad)\n",
    "            assert torch.all(-HyperboloidParameter.dot_h(x_,x_) > 0.0), f\"way off hyperboloid {torch.min(-HyperboloidParameter.dot_h(x_,x_))}\"\n",
    "        xxx = x_ / torch.sqrt(torch.clamp(-HyperboloidParameter.dot_h(x_,x_), min=0.0)).unsqueeze(-1)\n",
    "        return xxx\n",
    "        # return x / (-HyperboloidParameter.norm_h(x)).unsqueeze(-1)\n",
    "\n",
    "    def initial_proj(self):\n",
    "        \"\"\" Project the initialization of the embedding onto the manifold \"\"\"\n",
    "        self.data[...,0] = torch.sqrt(1 + torch.norm(self.data.detach()[...,1:],2,-1)**2)\n",
    "        self.proj()\n",
    "\n",
    "\n",
    "    def exp(self, lr):\n",
    "        \"\"\" Exponential map \"\"\"\n",
    "        x = self.data.detach()\n",
    "        # print(\"norm\", HyperboloidParameter.norm_h(x))\n",
    "        v = -lr * self.grad\n",
    "\n",
    "        retract = False\n",
    "        if retract:\n",
    "        # retraction\n",
    "            # print(\"retract\")\n",
    "            self.data = x + v\n",
    "\n",
    "        else:\n",
    "            assert torch.all(1 - torch.isnan(v))\n",
    "            n = self.__class__.norm_h(v).unsqueeze(-1)\n",
    "            assert torch.all(1 - torch.isnan(n))\n",
    "            n.clamp_(max=1.0)\n",
    "            # e = torch.cosh(n)*x + torch.sinh(n)*v/n\n",
    "            mask = torch.abs(n)<1e-7\n",
    "            cosh = torch.cosh(n)\n",
    "            cosh[mask] = 1.0\n",
    "            sinh = torch.sinh(n)\n",
    "            sinh[mask] = 0.0\n",
    "            n[mask] = 1.0\n",
    "            e = cosh*x + sinh/n*v\n",
    "            # assert torch.all(-HyperboloidParameter.dot_h(e,e) >= 0), torch.min(-HyperboloidParameter.dot_h(e,e))\n",
    "            self.data = e\n",
    "        self.proj()\n",
    "\n",
    "\n",
    "    def modify_grad_inplace(self):\n",
    "        \"\"\" Convert Euclidean gradient into Riemannian \"\"\"\n",
    "        self.grad[...,0] *= -1\n",
    "        self.grad -= self.__class__.dot_h(self.data, self.grad).unsqueeze(-1) / HyperboloidParameter.dot_h(self.data, self.data).unsqueeze(-1) * self.data\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, dist_fn, param_cls, n, d, project=True, initialize=None, learn_scale=False, initial_scale=0.0):\n",
    "        super().__init__()\n",
    "        self.dist_fn = dist_fn\n",
    "        self.n, self.d = n, d\n",
    "        self.project   = project\n",
    "        if initialize is not None: logging.info(f\"Initializing {np.any(np.isnan(initialize.numpy()))} {initialize.size()} {(n,d)}\")\n",
    "        self.w = param_cls(data=initialize, sizes=(n,d))\n",
    "        z =  torch.tensor([0.0], dtype=torch.double)\n",
    "        if learn_scale:\n",
    "            self.scale_log       = nn.Parameter(torch.tensor([initial_scale], dtype=torch.double))\n",
    "        else:\n",
    "            self.scale_log       = torch.tensor([initial_scale], dtype=torch.double, device=device)\n",
    "\n",
    "    def normalize(self):\n",
    "        self.w.proj()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainHyperbolic(input_tensor, ground_truth, n, encoder, encoder_optimizer, fc, fc_optimizer, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    fc_optimizer.zero_grad()\n",
    " \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = ground_truth.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    final_embeddings = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "    for idx in range(input_length):\n",
    "        output = fc(encoder_outputs[idx])\n",
    "        final_embeddings[idx] = output[0]\n",
    "\n",
    "    dist_recovered = distance_matrix_hyperbolic(final_embeddings) \n",
    "    loss += distortion(ground_truth, dist_recovered, n)\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    fc_optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does end to end hyperbolic.\n",
    "\n",
    "def trainHypIters(encoder, fc, n_iters=800, print_every=100, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "\n",
    "    encoder_optimizer = RiemannianSGD(encoder.parameters(), lr=learning_rate)\n",
    "    fc_optimizer = RiemannianSGD(fc.parameters(), lr=learning_rate)\n",
    "    training_pairs = [pairfromidx(idx) for idx in range(n_iters)]\n",
    "\n",
    "    for iter in range(1, n_iters + 1):     \n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_matrix = training_pair[1]\n",
    "        n = training_pair[2]\n",
    "        loss = trainHyperbolic(input_tensor, target_matrix, n, encoder, encoder_optimizer, fc, fc_optimizer)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
    "fc = nn.Linear(hidden_size, hidden_size).to(device)\n",
    "# trainHypIters(encoder, fc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
